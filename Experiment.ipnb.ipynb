{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to C:\\Users\\Gangotri\n",
      "[nltk_data]     Mishra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Gangotri\n",
      "[nltk_data]     Mishra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "data = gutenberg.raw('shakespeare-hamlet.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT SENTENCE INTO TOKENS\n",
    "from nltk.tokenize import word_tokenize\n",
    "data_token = word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT TOKENS INTO INTEGER\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "data_tokenizer = Tokenizer()\n",
    "\n",
    "# It create the vocabulary from the data that we are sharing\n",
    "# and count the frequency of each word\n",
    "# Building Frequency Dictionary\n",
    "data_tokenizer.fit_on_texts([data_token])\n",
    "\n",
    "#  After the vocabulary has been built using fit_on_texts, this method takes new texts (or the same ones)\n",
    "#  and replaces each token with its corresponding integer based on the vocabulary\n",
    "sequences = data_tokenizer.texts_to_sequences([data_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (open('data_tokenizer.pkl','wb')) as file:\n",
    "    pickle.dump(data_tokenizer,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.txt','w') as file:\n",
    "  file.write(data)\n",
    "\n",
    "with open('data.txt','r') as file:\n",
    "  text = file.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we will create sequential compbination of all words present in a sentence for all the sentences\n",
    "line_seq=[]\n",
    "for sentence in text.split('\\n'):\n",
    "  sent_seq = data_tokenizer.texts_to_sequences([sentence])[0]\n",
    "\n",
    "  for i in range(1,len(sent_seq)):\n",
    "    n_gram = sent_seq[:i+1]\n",
    "    line_seq.append(n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25227, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = max(len(x) for x in line_seq)\n",
    "pad_seq = pad_sequences(line_seq, maxlen= max_len, padding= 'pre')\n",
    "\n",
    "## Its an array\n",
    "pad_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x: creating independent feature\n",
    "# y: creating dependent feature\n",
    "x, y = pad_seq[:,:-1], pad_seq[:,-1]\n",
    "\n",
    "# total unique words\n",
    "total_words = len(data_tokenizer.word_index)\n",
    "\n",
    "# converting all values in dependet feature values in categories (one hotencode)\n",
    "y = tf.keras.utils.to_categorical(y, num_classes= total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state= 42, test_size= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-019cf05de8ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m## Specify the total words : which mean defnining total number of different words we have in voc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m## Use return_sequences= True: The LSTM layer returns the output for each time step in the input sequence, when you need the outputs for each time step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'total_words' is not defined"
     ]
    }
   ],
   "source": [
    "## Tran LSTM\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout,GRU\n",
    "## We can add GRU also instead of LSTM or with wil LSTM to0\n",
    "\n",
    "## Specify the total words : which mean defnining total number of different words we have in voc\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length= x_train.shape[1]))\n",
    "## Use return_sequences= True: The LSTM layer returns the output for each time step in the input sequence, when you need the outputs for each time step. \n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "## return_sequences=False (default): This is typically used for tasks where you only need the final output, such as classification tasks.\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation= 'softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 13, 100)           480700    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 13, 150)           150600    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 13, 150)           0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 150)               180600    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4807)              725857    \n",
      "=================================================================\n",
      "Total params: 1,537,757\n",
      "Trainable params: 1,537,757\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor = 'Val_loss', patience = 5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "631/631 [==============================] - 12s 18ms/step - loss: 4.9975 - accuracy: 0.1077 - val_loss: 7.4353 - val_accuracy: 0.0698\n",
      "Epoch 2/10\n",
      "631/631 [==============================] - 17s 28ms/step - loss: 4.8703 - accuracy: 0.1126 - val_loss: 7.6033 - val_accuracy: 0.0656\n",
      "Epoch 3/10\n",
      "631/631 [==============================] - 14s 22ms/step - loss: 4.7487 - accuracy: 0.1186 - val_loss: 7.7055 - val_accuracy: 0.0650\n",
      "Epoch 4/10\n",
      "631/631 [==============================] - 14s 21ms/step - loss: 4.6270 - accuracy: 0.1243 - val_loss: 7.7977 - val_accuracy: 0.0698\n",
      "Epoch 5/10\n",
      "631/631 [==============================] - 14s 22ms/step - loss: 4.5060 - accuracy: 0.1294 - val_loss: 8.0184 - val_accuracy: 0.0674\n",
      "Epoch 6/10\n",
      "631/631 [==============================] - 13s 21ms/step - loss: 4.3890 - accuracy: 0.1369 - val_loss: 8.1489 - val_accuracy: 0.0672\n",
      "Epoch 7/10\n",
      "631/631 [==============================] - 12s 19ms/step - loss: 4.2740 - accuracy: 0.1446 - val_loss: 8.3167 - val_accuracy: 0.0656\n",
      "Epoch 8/10\n",
      "631/631 [==============================] - 16s 26ms/step - loss: 4.1645 - accuracy: 0.1550 - val_loss: 8.4525 - val_accuracy: 0.0670\n",
      "Epoch 9/10\n",
      "631/631 [==============================] - 13s 21ms/step - loss: 4.0567 - accuracy: 0.1710 - val_loss: 8.6356 - val_accuracy: 0.0658\n",
      "Epoch 10/10\n",
      "631/631 [==============================] - 14s 23ms/step - loss: 3.9596 - accuracy: 0.1842 - val_loss: 8.7916 - val_accuracy: 0.0682\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs= 10, validation_data= (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 'lets go to the'\n",
    "def preprocessing_predict(inp):\n",
    "    ## we have to provide input string inside bracket otherwise data_tokenizer will treat each word separately and create token in separate lists\n",
    "    inp_seq = data_tokenizer.texts_to_sequences([inp])[0]\n",
    "    \n",
    "    if len(inp_seq) >= max_len:\n",
    "        inp_seq = inp_seq[-(max_len-1):]\n",
    "\n",
    "    ## in pad_sequence we do padding for multiple lists together that's why we put them in backet, so right now we have one list only but we will still put them in bracket\n",
    "    padded_in= pad_sequences([inp_seq],  padding= 'pre', maxlen= max_len-1)\n",
    "    pred = model.predict(padded_in)\n",
    "\n",
    "    ## getting maximum value which represents the most probable word\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    for word, index in data_tokenizer.word_index.items():\n",
    "        if index == pred:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king\n"
     ]
    }
   ],
   "source": [
    "print(preprocessing_predict(st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LSTM_predict.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
